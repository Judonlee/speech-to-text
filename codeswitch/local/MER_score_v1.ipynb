{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 7832\r\n",
      "-rw-r--r--   1 yehua  staff  326206 May  3 12:36 10.tra\r\n",
      "-rw-r--r--   1 yehua  staff  322138 May  3 12:36 11.tra\r\n",
      "-rw-r--r--   1 yehua  staff  317665 May  3 12:36 12.tra\r\n",
      "-rw-r--r--   1 yehua  staff  313328 May  3 12:36 13.tra\r\n",
      "-rw-r--r--   1 yehua  staff  309390 May  3 12:36 14.tra\r\n",
      "-rw-r--r--   1 yehua  staff  305906 May  3 12:36 15.tra\r\n",
      "-rw-r--r--   1 yehua  staff  302505 May  3 12:36 16.tra\r\n",
      "-rw-r--r--   1 yehua  staff  299137 May  3 12:36 17.tra\r\n",
      "-rw-r--r--   1 yehua  staff  342203 May  3 12:36 7.tra\r\n",
      "-rw-r--r--   1 yehua  staff  336249 May  3 12:36 8.tra\r\n",
      "-rw-r--r--   1 yehua  staff  331092 May  3 12:36 9.tra\r\n",
      "drwxrwxr-x  30 yehua  staff    1020 May  3 12:38 \u001b[1m\u001b[36mdecode\u001b[m\u001b[m/\r\n",
      "drwxrwxr-x  24 yehua  staff     816 May  3 12:36 \u001b[1m\u001b[36mlog\u001b[m\u001b[m/\r\n",
      "-rw-r--r--   1 yehua  staff  341386 May  3 12:36 test_filt.txt\r\n",
      "-rw-r--r--   1 yehua  staff  139160 May  3 12:37 words.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls -l ./scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NI45FBP_0101_2529230_2536840 9338 4946 4131 10104 11175 11147 6484 9157 9301 9311 8786 9616 8786 7663 11091 10794 8886 10938 10279 8771 \n",
      "NI45FBP_0101_2536840_2547460 8449 959 3581 7900 7758 3975 10938 8850 3940 7652 8337 3581 2210 6444 2 5489 9209 8850 7663 \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "less ../exp/tri2b/decode/scoring/17.tra | tail -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UI29FAZ_0104_0975410_0977779 后 悔 是 OK SIL154\n",
      "UI29FAZ_0104_0985128_0986293 ERR\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "less ../exp/tri2b/decode/scoring/test_filt.txt | tail -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<eps> 0\n",
      "<oov> 1\n",
      "A 2\n",
      "ABACK 3\n",
      "ABALONE 4\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "less ../data/lang/words.txt | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent path is /Volumes/STARTRACK/deep-learning\n"
     ]
    }
   ],
   "source": [
    "parent_path = os.path.split(os.getcwd())[0]\n",
    "print (\"parent path is {}\".format(parent_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_list(path, extension):\n",
    "    files = os.listdir(path)\n",
    "    files = [f for f in files if f.endswith(extension)]\n",
    "    print (files)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/STARTRACK/deep-learning/scoring'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tra_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints', 'kaldi-16-key', 'MER_score.ipynb', 'scoring']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(tra_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10.tra', '11.tra', '12.tra', '13.tra', '14.tra', '15.tra', '16.tra', '17.tra', '7.tra', '8.tra', '9.tra']\n"
     ]
    }
   ],
   "source": [
    "#tra_path = parent_path + \"/exp/tri2b/decode/scoring/\"\n",
    "tra_path = parent_path + \"/scoring/scoring/\"\n",
    "tra_files = get_file_list(tra_path, \".tra\") #need to get tri2b from terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_symbol_map():\n",
    "    r\"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    dic : defaultdict\n",
    "      a dictionary with key as word symbol and value as the corresponding word\n",
    "      e.g. '3': 'ABACK' \n",
    "    \"\"\"\n",
    "    dic = defaultdict(str)\n",
    "    # words_path = \"/data/lang/words.txt\"\n",
    "    words_path = \"/scoring/scoring/words.txt\"\n",
    "    with open(parent_path + words_path, \"r\") as inputfile:\n",
    "        for line in inputfile:\n",
    "            word = line.split(\" \")[0]\n",
    "            symbol = line.strip(\"\\n\").split(\" \")[1]\n",
    "            dic[symbol] = word\n",
    "    return dic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s2w = gen_symbol_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11360"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'因为'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2w['9338']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_ground_truth_map(model):\n",
    "    r\"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    dic : defaultdict\n",
    "      a dictionary with utterance id as key and groud truth transcript as its value \n",
    "    \"\"\"\n",
    "    dic = defaultdict(list)\n",
    "    #filt_path = \"/exp/\" + model + \"/decode/scoring/test_filt.txt\"\n",
    "    filt_path = \"/scoring/scoring/test_filt.txt\"\n",
    "    with open(parent_path + filt_path, \"r\") as inputfile:\n",
    "        for line in inputfile:\n",
    "            utterance_id = line.split(\" \")[0]\n",
    "            words = line.strip(\"\\n\").split(\" \")[1:]\n",
    "            dic[utterance_id] = words\n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "truth_map = gen_ground_truth_map(\"tri2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['不',\n",
       " '懂',\n",
       " 'BUT',\n",
       " 'OFFICIAL',\n",
       " 'RESULT',\n",
       " '还没有',\n",
       " '出',\n",
       " 'I',\n",
       " 'THINK',\n",
       " '出',\n",
       " '了',\n",
       " '他们',\n",
       " '就会']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth_map['NC01FBX_0101_0165090_0167860']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BUDGET', '那种', '出来', '得', '现在']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare two lists and find difference \n",
    "temp1 = ['那种', 'BUDGET', '现在', '还没有', '出来', '得', '出', '他们', '就会']# predicted\n",
    "temp2 = ['不','懂','BUT','OFFICIAL','RESULT','还没有','出','I','THINK','出','了','他们','就会'] # truth\n",
    "list(set(temp1) - set(temp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'他们', '出', '就会', '还没有'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare two lists and find match\n",
    "set(temp2).intersection(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isEnglish(word):\n",
    "    from nltk.corpus import wordnet\n",
    "    if word == \"SIL1\" or word == \"SIL2\":\n",
    "        return -1\n",
    "    if not wordnet.synsets(word):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isEnglish(\"look\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_single_sentence(sen1, sen2):\n",
    "    r\"\"\"sen2 as the ground truth, and sen1 as decoded label\n",
    "    Returns\n",
    "    -------\n",
    "    number of match,\n",
    "    number of english word match,\n",
    "    number of chinese word match,\n",
    "    number of silence phones match,\n",
    "    total number of the truth sentence,\n",
    "    total number of true words that are english,\n",
    "    total number of true words that are chinese,\n",
    "    total number of true words that are sil\n",
    "    \"\"\"\n",
    "    intersection = set(sen2).intersection(sen1)\n",
    "    # accuracy = len(intersection)/len(sen2)\n",
    "    \n",
    "    eng_inter = len([w for w in list(intersection) if isEnglish(w) == 1]) \n",
    "    # how many currect english word is captured \n",
    "    chi_inter = len([w for w in list(intersection) if isEnglish(w) == 0]) \n",
    "    # how many currect chinese word is capture\n",
    "    sil_inter = len([w for w in list(intersection) if isEnglish(w) == -1]) \n",
    "    # how many engligh are there in ground truth\n",
    "    total_true_eng = len([w for w in list(sen2) if isEnglish(w) == 1]) \n",
    "    # how many chinese are there in ground truth\n",
    "    total_true_chi = len([w for w in list(sen2) if isEnglish(w) == 0]) \n",
    "    # how many SIL are there in ground truth\n",
    "    total_true_sil = len([w for w in list(sen2) if isEnglish(w) == 0]) \n",
    "    \n",
    "    return [len(intersection), eng_inter, chi_inter, sil_inter, len(sen2),total_true_eng, total_true_chi,total_true_sil ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isEnglish(\"SIL1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wer0(r, h):\n",
    "    \"\"\"\n",
    "    credit to:\n",
    "    base author: Martin Thoma \n",
    "    url:https://martin-thoma.com/word-error-rate-calculation/\n",
    "    modifitication author: Emily Hua\n",
    "    \n",
    "    Calculation of WER with Levenshtein distance.\n",
    "\n",
    "    Works only for iterables up to 254 elements (uint8).\n",
    "    O(nm) time ans space complexity.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    r : list\n",
    "    h : list\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> wer(\"who is there\".split(), \"is there\".split())\n",
    "    1\n",
    "    >>> wer(\"who is there\".split(), \"\".split())\n",
    "    3\n",
    "    >>> wer(\"\".split(), \"who is there\".split())\n",
    "    3\n",
    "    \"\"\"\n",
    "    # initialisation\n",
    "    import numpy\n",
    "    \n",
    "    g_substitution = 0 # global \n",
    "    g_insertion = 0 \n",
    "    g_deletion = 0\n",
    "    \n",
    "    d = numpy.zeros((len(r)+1)*(len(h)+1), dtype=numpy.uint8)\n",
    "    d = d.reshape((len(r)+1, len(h)+1))\n",
    "    for i in range(len(r)+1):\n",
    "        for j in range(len(h)+1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "    #print(d)\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(r)+1):\n",
    "        for j in range(1, len(h)+1):\n",
    "            if r[i-1] == h[j-1]:\n",
    "                d[i][j] = d[i-1][j-1]\n",
    "            else:\n",
    "                substitution = d[i-1][j-1] + 1\n",
    "                insertion    = d[i][j-1] + 1\n",
    "                deletion     = d[i-1][j] + 1\n",
    "                \n",
    "                g_substitution += substitution\n",
    "                g_insertion += insertion\n",
    "                g_deletion += deletion\n",
    "                \n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    #print (d)\n",
    "\n",
    "    #return d[len(r)][len(h)], d[len(r)-1][len(h)-1], d[len(r)][len(h)-1], d[len(r)-1][len(h)]\n",
    "    return [d[len(r)][len(h)]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3392, 1)\n",
      "[31215]\n",
      "(3392, 1)\n",
      "[31199]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-9bcccd531fbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mtrue_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtruth_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m#print (true_words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mmerret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwer0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_words\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#r,h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;31m#print (merret)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m#print (merret)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-1458601be56b>\u001b[0m in \u001b[0;36mwer0\u001b[0;34m(r, h)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0msubstitution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0minsertion\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdeletion\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "variables = ['WER', 'Ins', 'Sub', 'Del', 'Eng Ins', 'Eng Sub', 'Eng Del',\n",
    "             'Che Ins', 'Che Sub', 'Che Del', 'Total Eng', 'Total Che']\n",
    "stacked = [0]*1\n",
    "stats_stacked = [0]*1\n",
    "counter = 0\n",
    "for f in tra_files:\n",
    "    with open(tra_path + f, \"r\") as inputfile:\n",
    "        stacked = [0]*1\n",
    "        for line in inputfile:\n",
    "            \n",
    "            #print (\"\\n\" + line)\n",
    "            symbols = line.replace(\"\\n\", \"\").split(\" \")[1:]\n",
    "            #print (symbols)\n",
    "            decoded_words = [s2w[sym] for sym in symbols][:-1] # remove \"\" caused by newline \n",
    "            #print (decoded_words)\n",
    "            true_words = truth_map[line.split(\" \")[0]]\n",
    "            #print (true_words)\n",
    "            merret = wer0(true_words, decoded_words) #r,h\n",
    "            #print (merret)\n",
    "            #print (merret)\n",
    "            #processed = [merret[var] for var in variables]\n",
    "            #print (processed)\n",
    "            stacked = np.vstack((stacked, merret))\n",
    "    stacked = stacked[1:]\n",
    "    print (stacked.shape)\n",
    "    stats = np.sum(stacked, axis=0)\n",
    "    print (stats)\n",
    "    stats_stacked = np.vstack((stats_stacked, stats))  \n",
    "# stats_stacked contains:    \n",
    "#[chi_WER, eng_WER, sil_WER, MER]\n",
    "stats_stacked = stats_stacked[1:]\n",
    "#print (stats_stacked)\n",
    "means = np.mean(stats_stacked, axis=0)\n",
    "# mins = np.min(stats_stacked, axis=0)\n",
    "# maxs = np.max(stats_stacked, axis=0)\n",
    "\n",
    "\n",
    "print (\"\\n average: MER {:.3f} \".format(means[0]))\n",
    "# print (\"\\n min: MER {:.3f}, eng_WER: {:.3f} , min_WER: {:.3f} \".format(mins[0], mins[1], mins[-1]))\n",
    "# print (\"\\n max: chi_WER {:.3f}, eng_WER: {:.3f} , max_WER: {:.3f} \".format(maxs[0], maxs[1], maxs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Che Cor: ': 0,\n",
       " 'Che Del': 0,\n",
       " 'Che Ins': 0,\n",
       " 'Che Sub': 0,\n",
       " 'Cor': 1,\n",
       " 'Del': 0,\n",
       " 'Eng Cor': 1,\n",
       " 'Eng Del': 0,\n",
       " 'Eng Ins': 1,\n",
       " 'Eng Sub': 2,\n",
       " 'Ins': 1,\n",
       " 'Sub': 2,\n",
       " 'Total Che': 0,\n",
       " 'Total Eng': 3,\n",
       " 'WER': 1.0}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = [\"what\", \"a\", \"bright\", \"day\"]\n",
    "r = [\"what\", \"a\", \"day\"]\n",
    "wer(r,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### functions ####\n",
    "# modified from https://segmentfault.com/q/1010000000732038\n",
    "# to detect english character and hyphen and ' and . ONLY\n",
    "def isAlphahyphen(word):\n",
    "    try:\n",
    "        float(word)\n",
    "        return True\n",
    "    except:\n",
    "        if word == '-':\n",
    "            return True\n",
    "        elif word == \"'\":\n",
    "            return True\n",
    "        elif word == \"'\":\n",
    "            return True   \n",
    "        else:\n",
    "            try:\n",
    "                return word.replace('-','').replace(\"'\",'').replace('.','').encode('ascii').isalnum()\n",
    "            except UnicodeEncodeError:\n",
    "                if word == '-':\n",
    "                    return True\n",
    "                else:\n",
    "                    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isAlphahyphen(\"Emily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"okay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split1():\n",
    "    test = [\"我知道了\", \"睡了\",\"okay\", \"fine\"]\n",
    "\n",
    "    chi = [list(ele) for ele in test if not isAlphahyphen(ele) and len(ele) > 1]\n",
    "    engs = [ele for ele in test if isAlphahyphen(ele)]\n",
    "    flattenchi = [item for sublist in chi for item in sublist]\n",
    "    engs.extend(flattenchi)\n",
    "    return (engs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8447518348693848\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for i in range(30000):\n",
    "    split1()\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-401c45927479>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'chi' is not defined"
     ]
    }
   ],
   "source": [
    "[item for sublist in chi for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split2(test):\n",
    "    '''split chinese words into character e.g. [\"我睡了\",\"okay\"] -> [\"我\", \"睡\", \"了\", \"okay\"]\n",
    "    Returns\n",
    "    -------\n",
    "    newr : list \n",
    "      a list of english word and chinese characters \n",
    "    '''\n",
    "    newr = []\n",
    "    for ele in test:\n",
    "        if not isAlphahyphen(ele) and len(ele) > 1:\n",
    "            newr += list(ele)\n",
    "        else:\n",
    "            newr += [ele]\n",
    "    return newr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = [\"我睡了\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16220402717590332\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for i in range(30000):\n",
    "    split2(test)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def single_sentence_mer(ref, hyp ,debug=False): #character level for Chinese\n",
    "#     r = ref.split()\n",
    "#     h = hyp.split()\n",
    "    # split chinese words into characters \n",
    "    r = split2(ref)\n",
    "    h = split2(hyp)\n",
    "    counter = 0\n",
    "#     r = ref\n",
    "#     h = hyp\n",
    "    totalEng = len([ele for ele in r if isAlphahyphen(ele)])\n",
    "    totalChe = len(r) - totalEng\n",
    "    #costs will holds the costs, like in the Levenshtein distance algorithm\n",
    "    costs = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
    "    # backtrace will hold the operations we've done.\n",
    "    # so we could later backtrace, like the WER algorithm requires us to.\n",
    "    backtrace = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
    " \n",
    "    OP_OK = 0\n",
    "    OP_SUB = 1\n",
    "    OP_INS = 2\n",
    "    OP_DEL = 3\n",
    "    SUB_PENALTY = 1\n",
    "    INS_PENALTY = 1\n",
    "    DEL_PENALTY = 1\n",
    "    # First column represents the case where we achieve zero\n",
    "    # hypothesis words by deleting all reference words.\n",
    "    for i in range(1, len(r)+1):\n",
    "        costs[i][0] = DEL_PENALTY*i\n",
    "        backtrace[i][0] = OP_DEL\n",
    "         \n",
    "    # First row represents the case where we achieve the hypothesis\n",
    "    # by inserting all hypothesis words into a zero-length reference.\n",
    "    for j in range(1, len(h) + 1):\n",
    "        costs[0][j] = INS_PENALTY * j\n",
    "        backtrace[0][j] = OP_INS\n",
    "     \n",
    "    # computation\n",
    "    for i in range(1, len(r)+1):\n",
    "        for j in range(1, len(h)+1):\n",
    "            if r[i-1] == h[j-1]:\n",
    "                costs[i][j] = costs[i-1][j-1]\n",
    "                backtrace[i][j] = OP_OK\n",
    "            else:\n",
    "                substitutionCost = costs[i-1][j-1] + SUB_PENALTY # penalty is always 1\n",
    "                insertionCost    = costs[i][j-1] + INS_PENALTY   # penalty is always 1\n",
    "                deletionCost     = costs[i-1][j] + DEL_PENALTY   # penalty is always 1\n",
    "                 \n",
    "                costs[i][j] = min(substitutionCost, insertionCost, deletionCost)\n",
    "                if costs[i][j] == substitutionCost:\n",
    "                    backtrace[i][j] = OP_SUB\n",
    "                elif costs[i][j] == insertionCost:\n",
    "                    backtrace[i][j] = OP_INS\n",
    "                else:\n",
    "                    backtrace[i][j] = OP_DEL\n",
    "                 \n",
    "    # back trace though the best route:\n",
    "    i = len(r)\n",
    "    j = len(h)\n",
    "    numSub = 0\n",
    "    numDel = 0\n",
    "    numIns = 0\n",
    "    numCor = 0\n",
    "    engOkay = 0\n",
    "    cheOkay = 0\n",
    "    engSub = 0\n",
    "    cheSub = 0\n",
    "    engDel = 0\n",
    "    cheDel = 0\n",
    "    engIns = 0\n",
    "    cheIns = 0\n",
    "    if debug:\n",
    "        print(\"OP\\tREF\\tHYP\")\n",
    "        lines = []\n",
    "    while i > 0 or j > 0:\n",
    "        if backtrace[i][j] == OP_OK:\n",
    "            numCor += 1\n",
    "            i-=1\n",
    "            j-=1\n",
    "            if isAlphahyphen(r[i]): # is english\n",
    "                if debug:\n",
    "                    lines.append(\"Eng OK\\t\" + r[i]+\"\\t\"+h[j])\n",
    "                engOkay += 1\n",
    "            else:\n",
    "                if debug:\n",
    "                    lines.append(\"Che OK\\t\" + r[i]+\"\\t\"+h[j])\n",
    "                cheOkay += 1 \n",
    "            #lines.append(\"OK\\t\" + r[i]+\"\\t\"+h[j])\n",
    "        elif backtrace[i][j] == OP_SUB:\n",
    "            numSub +=1\n",
    "            i-=1\n",
    "            j-=1\n",
    "            \n",
    "            if isAlphahyphen(r[i]): # is english\n",
    "                if debug:\n",
    "                    lines.append(\"Eng SUB\\t\" + r[i]+\"\\t\"+h[j])\n",
    "                engSub += 1 \n",
    "            else: \n",
    "                if debug:\n",
    "                    lines.append(\"Che SUB\\t\" + r[i]+\"\\t\"+h[j])\n",
    "                cheSub += 1\n",
    "            #lines.append(\"SUB\\t\" + r[i]+\"\\t\"+h[j])\n",
    "        elif backtrace[i][j] == OP_INS:\n",
    "            numIns += 1\n",
    "            j-=1\n",
    "            #print (r)\n",
    "            #print (h)\n",
    "            #print(\"i in insertion error {}, and r length is {}\".format (i, len(r)))\n",
    "            if i < len(r):\n",
    "                if isAlphahyphen(r[i]): # is english\n",
    "                    if debug:\n",
    "                        lines.append(\"Eng INS\\t\" + r[i]+\"\\t\"+h[j])\n",
    "                    engIns += 1\n",
    "                else: \n",
    "                    if debug:\n",
    "                        lines.append(\"Che INS\\t\" + r[i]+\"\\t\"+h[j])\n",
    "                    cheIns += 1\n",
    "            else:\n",
    "                \n",
    "                counter += 1\n",
    "                #print (counter)\n",
    "                if isAlphahyphen(r[i-1]):\n",
    "                     engIns += 1\n",
    "                else:\n",
    "                     cheIns += 1\n",
    "                \n",
    "            #lines.append(\"INS\\t\" + \"****\" + \"\\t\" + h[j])\n",
    "        elif backtrace[i][j] == OP_DEL:\n",
    "            numDel += 1\n",
    "            i-=1\n",
    "            if i < len(r):\n",
    "                if isAlphahyphen(r[i]): # is english\n",
    "                    if debug:\n",
    "                        lines.append(\"Eng DEL\\t\" + r[i]+\"\\t\"+h[j])\n",
    "                    engDel += 1\n",
    "                else: \n",
    "                    if debug:\n",
    "                        lines.append(\"Che DEL\\t\" + r[i]+\"\\t\"+h[j])\n",
    "                    cheDel += 1\n",
    "            else:\n",
    "                #print (counter)\n",
    "                counter += 1\n",
    "                if isAlphahyphen(r[i-1]):\n",
    "                    engDel += 1\n",
    "                else:\n",
    "                    cheDel += 1\n",
    "            #lines.append(\"DEL\\t\" + r[i]+\"\\t\"+\"****\")\n",
    "    if debug:\n",
    "        lines = reversed(lines)\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "        print(\"#cor \" + str(numCor))\n",
    "        print(\"#sub \" + str(numSub))\n",
    "        print(\"#del \" + str(numDel))\n",
    "        print(\"#ins \" + str(numIns))\n",
    "    #return (numSub + numDel + numIns) / (float) (len(r))\n",
    "    wer_result =  numSub + numDel + numIns\n",
    "    #print (\"index out of bound {} times\".format(counter))\n",
    "    return {'WER':wer_result, 'Cor':numCor, 'Sub':numSub, 'Ins':numIns, \n",
    "            'Del':numDel, \"Eng Sub\": engSub, \"Eng Ins\": engIns, \"Eng Del\": engDel, \"Eng Cor\": engOkay,\n",
    "             'Che Sub': cheSub, \"Che Ins\": cheIns, \"Che Del\": cheDel, \"Che Cor: \": cheOkay,\n",
    "           \"Total Eng\": totalEng, \"Total Che\": totalChe, \"out of bounds\" : counter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,20],[3,4],[99, 0]])\n",
    "np.argmax(x[:,0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Che Sub',\n",
       " 'Ins',\n",
       " 'Che Del',\n",
       " 'Del',\n",
       " 'Sub',\n",
       " 'Eng Sub',\n",
       " 'Total Che',\n",
       " 'Che Ins',\n",
       " 'Eng Ins',\n",
       " 'WER',\n",
       " 'Total Eng',\n",
       " 'Eng Del']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[var for var in variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sums:  [34041  2261 24100  7680  1241  7997  1627  1020 16103  6053 14277 43241]\n",
      "57518\n",
      "sums:  [34049  2065 23771  8213  1141  7888  1775   924 15883  6438 14277 43241]\n",
      "57518\n",
      "sums:  [34167  1853 23540  8774  1016  7849  1921   837 15691  6853 14277 43241]\n",
      "57518\n",
      "sums:  [34472  1715 23378  9379   953  7799  2095   762 15579  7284 14277 43241]\n",
      "57518\n",
      "sums:  [34727  1587 23175  9965   896  7696  2282   691 15479  7683 14277 43241]\n",
      "57518\n",
      "sums:  [34996  1440 23042 10514   816  7654  2438   624 15388  8076 14277 43241]\n",
      "57518\n",
      "sums:  [35274  1340 22880 11054   764  7587  2595   576 15293  8459 14277 43241]\n",
      "57518\n",
      "sums:  [35579  1228 22792 11559   712  7555  2716   516 15237  8843 14277 43241]\n",
      "57518\n",
      "sums:  [34901  3175 25630  6096  1690  8441  1246  1485 17189  4850 14277 43241]\n",
      "57518\n",
      "sums:  [34387  2819 24918  6650  1525  8230  1402  1294 16688  5248 14277 43241]\n",
      "57518\n",
      "sums:  [34097  2533 24441  7123  1388  8075  1524  1145 16366  5599 14277 43241]\n",
      "57518\n",
      "\n",
      " average: MER 0.602 \n",
      "\n",
      " min: MER 0.592, OVERALL INS 0.039, OVERALL SUB 0.419, OVERALL DEL 0.134,        ENG INS 0.022, ENG SUB 0.139,ENG DEL 0.028,         CHE INS 0.018, CHE SUB 0.280, CHE DEL 0.105)\n",
      "OVERALL  ENG      CHE          TYPE\n",
      "0.039   0.022   0.018       INS\n",
      "0.419   0.139   0.280       SUB\n",
      "0.134   0.028   0.105       DEL\n"
     ]
    }
   ],
   "source": [
    "variables_map  = {'WER':0, 'Ins':1, 'Sub':2, 'Del':3, 'Eng Ins':4, 'Eng Sub':5, 'Eng Del':6,\n",
    "             'Che Ins':7, 'Che Sub':8, 'Che Del':9, 'Total Eng':10, 'Total Che':11}\n",
    "variables = ['WER', 'Ins', 'Sub', 'Del', 'Eng Ins', 'Eng Sub', 'Eng Del',\n",
    "             'Che Ins', 'Che Sub', 'Che Del', 'Total Eng', 'Total Che']\n",
    "stacked = [0]*len(variables)\n",
    "stats_stacked = [0]*10\n",
    "counter = 0\n",
    "for f in tra_files:\n",
    "    with open(tra_path + f, \"r\") as inputfile:\n",
    "        stacked = [0]*len(variables)\n",
    "        for line in inputfile:\n",
    "            \n",
    "            #print (\"\\n\" + line)\n",
    "            symbols = line.replace(\"\\n\", \"\").split(\" \")[1:]\n",
    "            #print (symbols)\n",
    "            decoded_words = [s2w[sym] for sym in symbols][:-1] # remove \"\" caused by newline \n",
    "            #print (decoded_words)\n",
    "            true_words = truth_map[line.split(\" \")[0]]\n",
    "            #print (true_words)\n",
    "            merret = single_sentence_mer(true_words, decoded_words) #r,h\n",
    "            #print (merret)\n",
    "            processed = [merret[var] for var in variables]\n",
    "            #print (processed)\n",
    "            stacked = np.vstack((stacked, processed))\n",
    "    stacked = stacked[1:]\n",
    "    # print (stacked.shape)\n",
    "    stats = file_level_MER(stacked)\n",
    "    # print (stats)\n",
    "    stats_stacked = np.vstack((stats_stacked, stats))  \n",
    "# stats_stacked contains:    \n",
    "#[chi_WER, eng_WER, sil_WER, MER]\n",
    "stats_stacked = stats_stacked[1:]\n",
    "means = np.mean(stats_stacked, axis=0)\n",
    "maxs = np.max(stats_stacked, axis=0)\n",
    "mins_index = np.argmin(stats_stacked[:,variables_map['WER']], axis=0)\n",
    "min_MER = stats_stacked[mins_index][variables_map['WER']]\n",
    "min_ins = stats_stacked[mins_index][variables_map['Ins']]\n",
    "min_sub = stats_stacked[mins_index][variables_map['Sub']]\n",
    "min_del = stats_stacked[mins_index][variables_map['Del']]\n",
    "min_eng_ins = stats_stacked[mins_index][variables_map['Eng Ins']]\n",
    "min_eng_sub = stats_stacked[mins_index][variables_map['Eng Sub']]\n",
    "min_eng_del = stats_stacked[mins_index][variables_map['Eng Del']]\n",
    "min_che_ins = stats_stacked[mins_index][variables_map['Che Ins']]\n",
    "min_che_sub = stats_stacked[mins_index][variables_map['Che Sub']]\n",
    "min_che_del = stats_stacked[mins_index][variables_map['Che Del']]\n",
    "\n",
    "print (\"\\n average: MER {:.3f} \".format(means[0]))\n",
    "print (\"\\n min: MER {:.3f}, OVERALL INS {:.3f}, OVERALL SUB {:.3f}, OVERALL DEL {:.3f},\\\n",
    "        ENG INS {:.3f}, ENG SUB {:.3f},ENG DEL {:.3f}, \\\n",
    "        CHE INS {:.3f}, CHE SUB {:.3f}, CHE DEL {:.3f})\".format(\n",
    "        min_MER, min_ins, min_sub, min_del, min_eng_ins, min_eng_sub, min_eng_del, min_che_ins, min_che_sub,  min_che_del, mins[9])\n",
    "      )\n",
    "\n",
    "print ('{0:<8} {1:<8} {2:<8} {3:>8}'.format(\"OVERALL\", \"ENG\", \"CHE\", \"TYPE\"))\n",
    "    \n",
    "for args in ( (min_ins, min_eng_ins, min_che_ins, \"INS\" ), \n",
    "             (min_sub, min_eng_sub, min_che_sub, \"SUB\"), (min_del, min_eng_del, min_che_del, \"DEL\")):\n",
    "    print ('{0:.3f}   {1:.3f}   {2:.3f}  {3:>8}'.format(*args))\n",
    "# print (\"\\n min: MER {:.3f}, eng_WER: {:.3f} , min_WER: {:.3f} \".format(mins[0], mins[1], mins[-1]))\n",
    "# print (\"\\n max: chi_WER {:.3f}, eng_WER: {:.3f} , max_WER: {:.3f} \".format(maxs[0], maxs[1], maxs[-1]))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#isEnglish(\"emily\")\n",
    "isAlphahyphen(\"EMILY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer0(r,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OP\tREF\tHYP\n",
      "['好', '个', 'day']\n",
      "['what', '个', '好的', 'day']\n",
      "i in insertion error 2, and r length is 3\n",
      "Che SUB\t好\twhat\n",
      "Che OK\t个\t个\n",
      "Eng INS\tday\t好的\n",
      "Eng OK\tday\tday\n",
      "#cor 2\n",
      "#sub 1\n",
      "#del 0\n",
      "#ins 1\n",
      "CPU times: user 290 µs, sys: 34 µs, total: 324 µs\n",
      "Wall time: 344 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Che Cor: ': 1,\n",
       " 'Che Del': 0,\n",
       " 'Che Ins': 0,\n",
       " 'Che Sub': 1,\n",
       " 'Cor': 2,\n",
       " 'Del': 0,\n",
       " 'Eng Cor': 1,\n",
       " 'Eng Del': 0,\n",
       " 'Eng Ins': 1,\n",
       " 'Eng Sub': 0,\n",
       " 'Ins': 1,\n",
       " 'Sub': 1,\n",
       " 'Total Che': 2,\n",
       " 'Total Eng': 1,\n",
       " 'WER': 2}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = [\"what\", \"个\", \"好的\", \"day\"]\n",
    "r = [\"好\", \"个\", \"day\"]\n",
    "%time single_sentence_mer(r,h, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    var_map = {'WER': 0, 'Ins' : 1, 'Sub' : 2, 'Del' : 3, 'Eng Ins' : 4, 'Eng Sub' : 5, 'Eng Del' : 6,\n",
    "             'Che Ins' : 7, 'Che Sub' : 8, 'Che Del' : 9, 'Total Eng' : 10, 'Total Che' : 11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  var_map['WER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file_level_MER(stacked):\n",
    "    r\"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    ret : list\n",
    "       MER, sumed Ins, sumed Sub, sumed Del, sumed eng ins, \\ \n",
    "       sumed eng sub, sumed eng del, sumed che ins, sumed che sub, sumed che del on file level \n",
    "    \n",
    "    \"\"\"\n",
    "    var_map = {'WER': 0, 'Ins' : 1, 'Sub' : 2, 'Del' : 3, 'Eng Ins' : 4, 'Eng Sub' : 5, 'Eng Del' : 6,\n",
    "             'Che Ins' : 7, 'Che Sub' : 8, 'Che Del' : 9, 'Total Eng' : 10, 'Total Che' : 11}\n",
    "    stacked = stacked[1:]\n",
    "    sums = np.sum(stacked, axis=0)\n",
    "    print(\"sums: \", sums)\n",
    "    N = sums[var_map['Total Eng']] + sums[var_map['Total Che']]\n",
    "    print ( N)\n",
    "    eng_ins = sums[var_map['Eng Ins']] / N\n",
    "    eng_sub = sums[var_map['Eng Sub']] / N\n",
    "    eng_del = sums[var_map['Eng Del']] / N\n",
    "    che_ins = sums[var_map['Che Ins']] / N\n",
    "    che_sub = sums[var_map['Che Sub']] / N\n",
    "    che_del = sums[var_map['Che Del']] / N    \n",
    "    MER = sums[var_map['WER']] / N\n",
    "    ret = [MER, sums[var_map['Ins']]/N, sums[var_map['Sub']]/N, sums[var_map['Del']]/N, \n",
    "           eng_ins, eng_sub, eng_del, che_ins, che_sub, che_del]\n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8885829874428623"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0074081857825881361+ 0.21887248463195502+ 0.045794147007828509+ 0.0080071454841590915+ 0.48528345505175224+ 0.12321756948457942"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8885829874428623"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.88858298744286235"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [0.89253141680326142, 0.0060942293952002688, 0.21504223931408398, 0.05188500819568781, 0.0060732148110788885, 0.47574916992392718, 0.13768755516328332]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(a[1:]) == a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_file_stats(stacked):\n",
    "    stacked = stacked[1:]\n",
    "    sums = np.sum(stacked, axis=0)\n",
    "    \n",
    "    eng_WER = 1-(sums[1]/sums[-3])\n",
    "    chi_WER = 1-(sums[2]/sums[-2])\n",
    "    sil_WER = 1-(sums[3]/sums[-1])\n",
    "    MER = 1-(sums[0]/sums[-4])\n",
    "    return [chi_WER, eng_WER, sil_WER, MER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[isEnglish(i) for i in temp1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66320695916099837, 0.66906878802058967, 1.0, 0.66452317908628589]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-215-e3f51c109e01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mtrue_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtruth_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;31m#print (true_words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_single_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0mstacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mstacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstacked\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-20cc15f9436d>\u001b[0m in \u001b[0;36mprocess_single_sentence\u001b[0;34m(sen1, sen2)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtotal_true_chi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misEnglish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# how many SIL are there in ground truth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtotal_true_sil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misEnglish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meng_inter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchi_inter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msil_inter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_true_eng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_true_chi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_true_sil\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-20cc15f9436d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtotal_true_chi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misEnglish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# how many SIL are there in ground truth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtotal_true_sil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misEnglish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meng_inter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchi_inter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msil_inter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_true_eng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_true_chi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_true_sil\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-a5ac97647b50>\u001b[0m in \u001b[0;36misEnglish\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"SIL1\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"SIL2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36msynsets\u001b[0;34m(self, lemma, pos, lang)\u001b[0m\n\u001b[1;32m   1422\u001b[0m                 \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPOS_LIST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             return [get_synset(p, offset)\n\u001b[0;32m-> 1424\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m                     for offset in index[form].get(p, [])]\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1423\u001b[0m             return [get_synset(p, offset)\n\u001b[1;32m   1424\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m                     for offset in index[form].get(p, [])]\n\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[0;34m(self, form, pos)\u001b[0m\n\u001b[1;32m   1694\u001b[0m                     if form.endswith(old)]\n\u001b[1;32m   1695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1696\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mfilter_forms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1697\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0mseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stacked = [0]*8\n",
    "stats_stacked = [0]*4\n",
    "counter = 0\n",
    "for f in tra_files:\n",
    "    with open(tra_path + f, \"r\") as inputfile:\n",
    "        for line in inputfile:\n",
    "                #print (line)\n",
    "                symbols = line.replace(\"\\n\", \"\").split(\" \")[1:]\n",
    "                #print (symbols)\n",
    "                decoded_words = [s2w[sym] for sym in symbols][:-1] # remove \"\" caused by newline \n",
    "                #print (decoded_words )\n",
    "                true_words = truth_map[line.split(\" \")[0]]\n",
    "                #print (true_words)\n",
    "                processed = process_single_sentence(decoded_words, true_words)\n",
    "                stacked = np.vstack((stacked, processed))\n",
    "    stacked = stacked[1:]\n",
    "    stats = gen_file_stats(stacked)\n",
    "    print (stats)\n",
    "    stats_stacked = np.vstack((stats_stacked, stats))\n",
    "    \n",
    "# stats_stacked contains:    \n",
    "#[chi_WER, eng_WER, sil_WER, MER]\n",
    "stats_stacked = stats_stacked[1:]\n",
    "means = np.mean(stats_stacked, axis=0)\n",
    "chi_WER_mean = means[0]\n",
    "eng_WER_mean = means[1]\n",
    "sil_WER_mean = means[-2]\n",
    "WER_mean = means[-1]\n",
    "mins = np.min(stats_stacked, axis=0)\n",
    "maxs = np.max(stats_stacked, axis=0)\n",
    "\n",
    "print (\"\\n average: chi_WER {:.3f}, eng_WER: {:.3f} , mean_WER: {:.3f} \".format(chi_WER_mean, eng_WER_mean, WER_mean))\n",
    "print (\"\\n min: chi_WER {:.3f}, eng_WER: {:.3f} , min_WER: {:.3f} \".format(mins[0], mins[1], mins[-1]))\n",
    "print (\"\\n max: chi_WER {:.3f}, eng_WER: {:.3f} , max_WER: {:.3f} \".format(maxs[0], maxs[1], maxs[-1]))\n",
    "                \n",
    "            \n",
    "                          \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sil_WER_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
