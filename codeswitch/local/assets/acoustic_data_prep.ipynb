{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__=\"Emily Hua\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have to create some text files that will allow Kaldi to communicate with your audio data. Consider these files as 'must be done'. Each file that you will create in this section (and in Language data section as well) can be considered as a text file with some number of strings (each string in a new line). These strings need to be sorted. If you will encounter any sorting issues you can use Kaldi scripts for checking (utils/validate_data_dir.sh) and fixing (utils/fix_data_dir.sh) data order. And for you information - utils directory will be attached to your project in Tools attachment section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/STARTRACK/deep-learning/code-switch/speech-to-text/codeswitch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "parent_path = os.path.split(os.getcwd())[0]\n",
    "print (parent_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In kaldi/egs/code-switch directory, create a folder **data**. Then create **test** and **train** subfolders inside. Create in each subfolder following files (so you have files named in the same way in test and train subfolders but they relate to two different data sets that you created before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory = parent_path + \"/data/train\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory = parent_path + \"/data/test\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n",
      "drwxr-xr-x  3 yehua  staff  102 Mar 29 14:50 \u001b[1m\u001b[36mlocal\u001b[m\u001b[m/\r\n",
      "drwxr-xr-x  8 yehua  staff  272 Mar 29 18:07 \u001b[1m\u001b[36mtest\u001b[m\u001b[m/\r\n",
      "drwxr-xr-x  8 yehua  staff  272 Mar 29 14:52 \u001b[1m\u001b[36mtrain\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls -l ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 19648\r\n",
      "-rw-r--r--  1 yehua  staff  2982565 Mar 29 18:51 segments\r\n",
      "-rw-r--r--  1 yehua  staff     1600 Mar 29 18:48 spk2gender\r\n",
      "-rw-r--r--  1 yehua  staff  5133989 Mar 29 18:49 text\r\n",
      "-rw-r--r--  1 yehua  staff  1892192 Mar 29 18:50 utt2spk\r\n",
      "-rw-r--r--  1 yehua  staff    38558 Mar 29 18:48 wav.scp\r\n"
     ]
    }
   ],
   "source": [
    "ls -l ../data/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1664\r\n",
      "-rw-r--r--  1 yehua  staff  239939 Mar 29 18:51 segments\r\n",
      "-rw-r--r--  1 yehua  staff     140 Mar 29 18:48 spk2gender\r\n",
      "-rw-r--r--  1 yehua  staff  446200 Mar 29 18:49 text\r\n",
      "-rw-r--r--  1 yehua  staff  153069 Mar 29 18:50 utt2spk\r\n",
      "-rw-r--r--  1 yehua  staff    2740 Mar 29 18:48 wav.scp\r\n"
     ]
    }
   ],
   "source": [
    "ls -l ../data/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a.) spk2gender \n",
    "This file informs about speakers gender. As we assumed, 'speakerID' is a unique name of each speaker.\n",
    "\n",
    "Pattern: [speakerID] [gender]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are total 210 of files in /Volumes/STARTRACK/deep-learning/code-switch/speech-to-text/codeswitch/LDC2015S04/seame_d2/data/interview/audio\n",
      "\n",
      "there are total 87 of files in /Volumes/STARTRACK/deep-learning/code-switch/speech-to-text/codeswitch/LDC2015S04/seame_d1/data/conversation/audio\n",
      "\n",
      "there are 95 unique prefix sets\n",
      "there are 79 unique prefix sets\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict \n",
    "\n",
    "def count_files(audio_path):\n",
    "    dir_list = os.listdir(audio_path)[1:]\n",
    "    print (\"there are total {} of files in {}\\n\".format(len(dir_list), audio_path))\n",
    "    return dir_list\n",
    "audio_path_i = parent_path + '/LDC2015S04/seame_d2/data/interview/audio'\n",
    "audio_path_c = parent_path + '/LDC2015S04/seame_d1/data/conversation/audio'\n",
    "dir_list_i = count_files(audio_path_i)\n",
    "dir_list_c = count_files(audio_path_c)\n",
    "\n",
    "\n",
    "def speaker_re_counts(dir_list):\n",
    "    id_dic = defaultdict(int)\n",
    "    for file in dir_list:\n",
    "        id_dic[re.split('_', file)[0]] += 1\n",
    "    print ('there are {} unique prefix sets'.format(len(id_dic)))\n",
    "    return id_dic\n",
    "id_dic_i = speaker_re_counts(dir_list_i)\n",
    "id_dic_c = speaker_re_counts(dir_list_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 85 speaker ids in the training set\n",
      "there are 10 speaker ids in the testing set\n",
      "\n",
      "there are 75 speaker ids in the training set\n",
      "there are 4 speaker ids in the testing set\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_short_ids_i = ['01MA', '03FA','08MA', '29FA','29MB','42FB','44MB','45FB','67MB','55FB']\n",
    "test_short_ids_c = ['01NC01FB', '01NC02FB','06NC11MA', '06NC12MA']\n",
    "\n",
    "def train_test_split(id_dic, test_short_ids):\n",
    "    train_ids = []\n",
    "    test_ids = []\n",
    "    for key in id_dic:\n",
    "        if key[2:-1] in test_short_ids or key[:-1] in test_short_ids:\n",
    "            test_ids.append(key)\n",
    "        else: \n",
    "            train_ids.append(key)\n",
    "    print ('there are {} speaker ids in the training set'.format(len(train_ids)))\n",
    "    print ('there are {} speaker ids in the testing set\\n'.format(len(test_ids)))\n",
    "    return train_ids, test_ids\n",
    "train_ids_i, test_ids_i = train_test_split(id_dic_i, test_short_ids_i)\n",
    "train_ids_c, test_ids_c = train_test_split(id_dic_c, test_short_ids_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NI55FBP', 'NI42FBQ', 'UI03FAZ', 'NI29MBP', 'NI67MBQ']\n",
      "['NI30MBQ', 'UI28FAZ', 'NI27MBQ', 'NI31FBP', 'NI08FBP']\n",
      "['06NC11MAX', '01NC02FBY', '06NC12MAY', '01NC01FBX']\n",
      "['25NC43FBQ', '17NC34FBQ', '04NC07FBX', '23NC45MBP', '22NC44MBQ']\n"
     ]
    }
   ],
   "source": [
    "print (test_ids_i[:5])\n",
    "print (train_ids_i[:5])\n",
    "print (test_ids_c[:5])\n",
    "print (train_ids_c[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent path: /Volumes/STARTRACK/deep-learning/code-switch/speech-to-text/codeswitch\n"
     ]
    }
   ],
   "source": [
    "print (\"parent path: {}\".format(parent_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add interview speaker id to the train \n",
    "directory = parent_path + \"/data/train/spk2gender\"\n",
    "with open(directory, 'w') as outfile:\n",
    "    for speakerid in train_ids_i:\n",
    "        outfile.write(\"{} {}\\n\".format(speakerid,speakerid[4].lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add conversation speaker id to the train \n",
    "directory = parent_path + \"/data/train/spk2gender\"\n",
    "with open(directory, 'a+') as outfile:\n",
    "    for speakerid in train_ids_c:\n",
    "        outfile.write(\"{} {}\\n\".format(speakerid[2:],speakerid[6].lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add interview speaker id to the test \n",
    "directory = parent_path + \"/data/test/spk2gender\"\n",
    "with open(directory, 'w') as outfile:\n",
    "    for speakerid in test_ids_i:\n",
    "        outfile.write(\"{} {}\\n\".format(speakerid, speakerid[4].lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish creating spk2gender in train and test set \n"
     ]
    }
   ],
   "source": [
    "# add conversation speaker id to the test \n",
    "directory = parent_path + \"/data/test/spk2gender\"\n",
    "with open(directory, 'a+') as outfile:\n",
    "    for speakerid in test_ids_c:\n",
    "        outfile.write(\"{} {}\\n\".format(speakerid[2:], speakerid[6].lower()))\n",
    "        \n",
    "print (\"finish creating spk2gender in train and test set \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "less ../data/test/spk2gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "less ../data/train/spk2gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.) wav.scp \n",
    "This file connects every utterance (sentence said by one person during particular recording session) with an audio file related to this utterance. If you stick to my naming approach, 'utteranceID' is nothing more than 'speakerID' (speaker's folder name) glued with *.wav file name without '.wav' ending (look for examples below).\n",
    "\n",
    "Pattern: [recordingID] [full_path_to_audio_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 35456\r\n",
      "-rw-r--r--  1 yehua  staff  18149573 Mar 29 00:29 UI03FAZ_0101.flac\r\n"
     ]
    }
   ],
   "source": [
    "ls -l ../interview_audio/test/UI03FAZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add interview train into wav.scp \n",
    "\n",
    "directory = parent_path + \"/data/train/wav.scp\"\n",
    "with open(directory, 'w') as outfile:\n",
    "    for file in dir_list_i:\n",
    "        speaker_id = re.split(\"_\", file)[0]\n",
    "        if speaker_id in train_ids_i:\n",
    "            path = parent_path + \"/audio/train/\" + speaker_id + \"/\" + file\n",
    "            outfile.write(\"{} flac -c -d -s {} |\\n\".format(re.split(\"\\.\", file)[0], path))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to carefully re-name recordings in conversation, cause they don't have the speaker id as prefix. In their original naming convension, recordings from the same speaker will be identifies as different speaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['25NC43FBQ']\n"
     ]
    }
   ],
   "source": [
    "print(train_ids_c[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add conversation train into wav.scp \n",
    "speaker_multiple = ['NC50FBP', 'NC44MBQ', 'NC45MBP', 'NC05FAX', 'NC49FBQ', 'NC41MBP', 'NC07FBX', 'NC03FBX', 'NC04FBY', 'NC10MAY', 'NC37MBP', 'NC36MBQ', 'NC35FBQ', 'NC22MBQ', 'NC08FBY', 'NC48FBP', 'NC06FAY', 'NC43FBQ', 'NC09FAX']\n",
    "directory = parent_path + \"/data/train/wav.scp\"\n",
    "with open(directory, 'a+') as outfile:\n",
    "    for file in dir_list_c:\n",
    "        speaker_id = re.split(\"_\", file)[0]\n",
    "        if speaker_id in train_ids_c:\n",
    "            if speaker_id[2:] in speaker_multiple:\n",
    "                pre = re.split(\"_\",file)[0][:2]\n",
    "                end = re.split(\"_\",file)[1].split(\".\")[0]\n",
    "                newfile = re.split(\"_\",file)[0][2:] + '_' + pre + end + \".flac\"\n",
    "                path = parent_path + \"/audio/train/\" + speaker_id[2:] + \"/\" + newfile\n",
    "                recording_id = re.split(\"\\.\", newfile)[0]\n",
    "                #print (\"this {} has multiple recordings, renaming them to {}\".format(speaker_id, newfile))\n",
    "            else: \n",
    "                path = parent_path + \"/audio/train/\" + speaker_id[2:] + \"/\" + file[2:]\n",
    "                recording_id = re.split(\"\\.\", file)[0][2:]\n",
    "            outfile.write(\"{} flac -c -d -s {} |\\n\".format(recording_id, path))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add interview test into wav.scp \n",
    "\n",
    "directory = parent_path + \"/data/test/wav.scp\"\n",
    "with open(directory, 'w') as outfile:\n",
    "    for file in dir_list_i:\n",
    "        speaker_id = re.split(\"_\", file)[0]\n",
    "        if speaker_id in test_ids_i:\n",
    "            path = parent_path + \"/audio/test/\" + speaker_id + \"/\" + file\n",
    "            outfile.write(\"{} flac -c -d -s {} |\\n\".format(re.split(\"\\.\", file)[0], path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['06NC11MAX', '01NC02FBY', '06NC12MAY', '01NC01FBX']\n"
     ]
    }
   ],
   "source": [
    "print (test_ids_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish creating wav.scp in train and test set\n"
     ]
    }
   ],
   "source": [
    "# add conversation test into wav.scp \n",
    "\n",
    "directory = parent_path + \"/data/test/wav.scp\"\n",
    "with open(directory, 'a+') as outfile:\n",
    "    for file in dir_list_c:\n",
    "        speaker_id = re.split(\"_\", file)[0]\n",
    "        if speaker_id in test_ids_c:\n",
    "            path = parent_path + \"/audio/test/\" + speaker_id[2:] + \"/\" + file[2:]\n",
    "            outfile.write(\"{} flac -c -d -s {} |\\n\".format(re.split(\"\\.\", file)[0][2:], path))\n",
    "print (\"finish creating wav.scp in train and test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "less ../data/test/wav.scp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "less ../data/train/wav.scp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c.) text \n",
    "This file contains every utterance matched with its text transcription.\n",
    "\n",
    "Pattern: [uterranceID] [text_transcription]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NI01MAX_0101.txt', 'NI02FAX_0101.txt']\n",
      "['01NC01FBX_0101.txt', '01NC02FBY_0101.txt']\n"
     ]
    }
   ],
   "source": [
    "trans_path_i = parent_path + \"/LDC2015S04/seame_d2/data/interview/transcript\"\n",
    "trans_list_i = os.listdir(trans_path_i)[1:]\n",
    "trans_path_c = parent_path + \"/LDC2015S04/seame_d1/data/conversation/transcript\"\n",
    "trans_list_c = os.listdir(trans_path_c)[1:]\n",
    "print (trans_list_i[:2])\n",
    "print (trans_list_c[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create interview text in train set \n",
    "\n",
    "directory = parent_path + \"/data/train/text\"\n",
    "with open(directory, 'w') as outputfile:\n",
    "    for file in trans_list_i: \n",
    "        speaker_id = re.split(\"_\", file)[0]\n",
    "        if speaker_id in train_ids_i:\n",
    "            trans_file = trans_path_i + \"/\" + file\n",
    "            with open(trans_file, 'r') as inputfile:\n",
    "                for line in inputfile:\n",
    "                    text = re.split(\"\\t\", line)[-1]\n",
    "                    prefix = re.split(\"\\t\", line)[0]\n",
    "                    first_frame = re.split(\"\\t\", line)[1].zfill(7)\n",
    "                    second_frame = re.split(\"\\t\", line)[2].zfill(7)\n",
    "                    utterance_id = prefix + \"_\" + first_frame + \"_\" + second_frame\n",
    "                    outputfile.write(\"{} {}\".format(utterance_id, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create conversation text in train set \n",
    "\n",
    "directory = parent_path + \"/data/train/text\"\n",
    "counter = 0 \n",
    "with open(directory, 'a+') as outputfile:\n",
    "    for file in trans_list_c: \n",
    "        speaker_id = re.split(\"_\", file)[0]\n",
    "        if speaker_id in train_ids_c:\n",
    "            trans_file = trans_path_c + \"/\" + file\n",
    "            with open(trans_file, 'r') as inputfile:\n",
    "                for line in inputfile:\n",
    "                    if speaker_id[2:] in speaker_multiple:\n",
    "                        pre = re.split(\"_\",file)[0][:2]\n",
    "                        end = re.split(\"_\",file)[1].split(\".\")[0]\n",
    "                        prefix = re.split(\"_\",file)[0][2:] + '_' + pre + end\n",
    "                        #print (\"this speaker {} has multiple recordings, renaming it to {}\".format(speaker_id, prefix))\n",
    "\n",
    "                    else: \n",
    "                        prefix = re.split(\"\\t\", line)[0][2:]\n",
    "                    text = re.split(\"\\t\", line)[-1]\n",
    "                    first_frame = re.split(\"\\t\", line)[1].zfill(7)\n",
    "                    second_frame = re.split(\"\\t\", line)[2].zfill(7)\n",
    "                    utterance_id = prefix + \"_\" + first_frame + \"_\" + second_frame\n",
    "                    outputfile.write(\"{} {}\".format(utterance_id, text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "less ../data/train/text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create interview text in test \n",
    "\n",
    "directory = parent_path + \"/data/test/text\"\n",
    "with open(directory, 'w') as outputfile:\n",
    "    for file in trans_list_i: \n",
    "        speaker_id = re.split(\"_\", file)[0]\n",
    "        if speaker_id in test_ids_i:\n",
    "            trans_file = trans_path_i + \"/\" + file\n",
    "            with open(trans_file, 'r') as inputfile:\n",
    "                for line in inputfile:\n",
    "                    text = re.split(\"\\t\", line)[-1]\n",
    "                    prefix = re.split(\"\\t\", line)[0]\n",
    "                    first_frame = re.split(\"\\t\", line)[1].zfill(7)\n",
    "                    second_frame = re.split(\"\\t\", line)[2].zfill(7)\n",
    "                    utterance_id = prefix + \"_\" + first_frame + \"_\" + second_frame\n",
    "                    outputfile.write(\"{} {}\".format(utterance_id, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish create text in train and test set\n"
     ]
    }
   ],
   "source": [
    "# create conversation text in test \n",
    "\n",
    "directory = parent_path + \"/data/test/text\"\n",
    "with open(directory, 'a+') as outputfile:\n",
    "    for file in trans_list_c: \n",
    "        speaker_id = re.split(\"_\", file)[0]\n",
    "        if speaker_id in test_ids_c:\n",
    "            trans_file = trans_path_c + \"/\" + file\n",
    "            with open(trans_file, 'r') as inputfile:\n",
    "                for line in inputfile:\n",
    "                    text = re.split(\"\\t\", line)[-1]\n",
    "                    prefix = re.split(\"\\t\", line)[0][2:]\n",
    "                    first_frame = re.split(\"\\t\", line)[1].zfill(7)\n",
    "                    second_frame = re.split(\"\\t\", line)[2].zfill(7)\n",
    "                    utterance_id = prefix + \"_\" + first_frame + \"_\" + second_frame\n",
    "                    outputfile.write(\"{} {}\".format(utterance_id, text))\n",
    "                    \n",
    "print (\"finish create text in train and test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "less ../data/test/text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d.) utt2spk \n",
    "This file tells the ASR system which utterance belongs to particular speaker.\n",
    "\n",
    "Pattern: [uterranceID] [speakerID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "#trans_path = parent_path + \"/LDC2015S04/seame_d2/data/interview/transcript/\"\n",
    "import sys\n",
    "trans_list = os.listdir(trans_path_i)[1:]\n",
    "largest_frame = -sys.maxsize\n",
    "print (len(trans_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'NI01MAX_0101.txt' in trans_list_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 210 recording with transcript\n",
      "there are 210 recording in audio files\n",
      "match!\n",
      "largest_frame is 7004497\n",
      "since 7004497 is our largest frame, then we need to create string with 7 digits to hold all frames\n"
     ]
    }
   ],
   "source": [
    "#trans_path = parent_path + \"/LDC2015S04/seame_d2/data/interview/transcript/\"\n",
    "import sys\n",
    "largest_frame = -sys.maxsize\n",
    "trans_list_i = os.listdir(trans_path_i)[1:]\n",
    "print (\"there are {} recording with transcript\".format(len(trans_list_i)))\n",
    "print (\"there are {} recording in audio files\".format(len(dir_list_i)))\n",
    "print (\"match!\")\n",
    "\n",
    "for file in trans_list_i:\n",
    "    file_path = trans_path_i + \"/\" + file\n",
    "    with open(file_path, 'r') as inputfile:\n",
    "        for line in inputfile:\n",
    "            second_frame = int(re.split(\"\\t\", line)[2])\n",
    "            if second_frame > largest_frame:\n",
    "                largest_frame = second_frame\n",
    "print (\"largest_frame is {}\".format(largest_frame))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for file in trans_list_c:\n",
    "    file_path = trans_path_c + \"/\" + file\n",
    "    with open(file_path, 'r') as inputfile:\n",
    "        for line in inputfile:\n",
    "            second_frame = int(re.split(\"\\t\", line)[2])\n",
    "            if second_frame > largest_frame:\n",
    "                largest_frame = second_frame\n",
    "print (\"largest_frame is {}\".format(largest_frame))  \n",
    "print (\"since 7242490 is our largest frame, then we need to create string with 7 digits to hold all frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 40712 new utterance ids\n"
     ]
    }
   ],
   "source": [
    "# create utterance id for interview: recording id + start time + end time; for e.g. NI01MAX_0101_0001353_0003612\n",
    "counter = 0\n",
    "utter_ids_i = []\n",
    "for file in trans_list_i:\n",
    "    file_path = trans_path_i + \"/\" + file\n",
    "    with open(file_path, 'r') as inputfile:\n",
    "        for line in inputfile:\n",
    "            speaker_id = re.split(\"_\", line)[0]\n",
    "            prefix = re.split(\"\\t\", line)[0]\n",
    "            first_frame = re.split(\"\\t\", line)[1].zfill(7)\n",
    "            second_frame = re.split(\"\\t\", line)[2].zfill(7)\n",
    "            utterance_id =  prefix + \"_\" + first_frame + \"_\" + second_frame\n",
    "            utter_ids_i.append(utterance_id)\n",
    "print (\"there are {} new utterance ids\".format(len(utter_ids_i)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 14217 new utterance ids\n"
     ]
    }
   ],
   "source": [
    "# create utterance id for conversation\n",
    "\n",
    "counter = 0\n",
    "utter_ids_c = []\n",
    "for file in trans_list_c:\n",
    "    file_path = trans_path_c + \"/\" + file\n",
    "    with open(file_path, 'r') as inputfile:\n",
    "        for line in inputfile:\n",
    "            speaker_id = re.split(\"_\", line)[0]\n",
    "            if speaker_id[2:] in speaker_multiple:\n",
    "                pre = re.split(\"_\",file)[0][:2]\n",
    "                end = re.split(\"_\",file)[1].split(\".\")[0]\n",
    "                prefix = re.split(\"_\",file)[0][2:] + '_' + pre + end\n",
    "            else:\n",
    "                prefix = re.split(\"\\t\", line)[0][2:]\n",
    "            first_frame = re.split(\"\\t\", line)[1].zfill(7)\n",
    "            second_frame = re.split(\"\\t\", line)[2].zfill(7)\n",
    "            utterance_id =  prefix + \"_\" + first_frame + \"_\" + second_frame\n",
    "            utter_ids_c.append(utterance_id)\n",
    "print (\"there are {} new utterance ids\".format(len(utter_ids_c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample newly created utterance id ['NI01MAX_0101_0001353_0003612']\n"
     ]
    }
   ],
   "source": [
    "print(\"sample newly created utterance id {}\".format(utter_ids_i[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample newly created utterance id ['NC01FBX_0101_0086300_0088370']\n"
     ]
    }
   ],
   "source": [
    "print(\"sample newly created utterance id {}\".format(utter_ids_c[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ids_c_short = [ x[2:] for x in train_ids_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NC43FBQ', 'NC34FBQ']\n"
     ]
    }
   ],
   "source": [
    "print (train_ids_c_short[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 50792 in train/utt2spk\n"
     ]
    }
   ],
   "source": [
    "utt2spk_path = parent_path + \"/data/train/utt2spk\"\n",
    "counter = 0\n",
    "with open(utt2spk_path, 'w') as outputfile:\n",
    "    for file in utter_ids_i:\n",
    "        speaker_id = re.split(\"_\", file)[0]\n",
    "        if speaker_id in train_ids_i:\n",
    "            counter += 1\n",
    "            outputfile.write(\"{} {}\\n\".format(file, speaker_id))\n",
    "    for file in utter_ids_c:\n",
    "        speaker_id = re.split(\"_\",file)[0]\n",
    "        if speaker_id in train_ids_c_short:\n",
    "            counter += 1\n",
    "            outputfile.write(\"{} {}\\n\".format(file, speaker_id))\n",
    "print (\"there are {} in train/utt2spk\".format(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['06NC11MAX', '01NC02FBY', '06NC12MAY', '01NC01FBX']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ids_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 4137 in test/utt2spk\n"
     ]
    }
   ],
   "source": [
    "test_ids_c_short = [x[2:] for x in test_ids_c]\n",
    "utt2spk_path = parent_path + \"/data/test/utt2spk\"\n",
    "counter = 0\n",
    "with open(utt2spk_path, 'w') as outputfile:\n",
    "    for file in utter_ids_i:\n",
    "        speaker_id = re.split(\"_\", file)[0]\n",
    "        if speaker_id in test_ids_i:\n",
    "            counter += 1\n",
    "            outputfile.write(\"{} {}\\n\".format(file, speaker_id))\n",
    "    for file in utter_ids_c:\n",
    "        speaker_id = re.split(\"_\", file)[0]\n",
    "        if speaker_id in test_ids_c_short:\n",
    "            counter += 1\n",
    "            outputfile.write(\"{} {}\\n\".format(file, speaker_id))\n",
    "print (\"there are {} in test/utt2spk\".format(counter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "less ../data/train/utt2spk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "less ../data/test/utt2spk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.) corpus.txt \n",
    "This file has a slightly different directory. In kaldi-trunk/egs/digits/data create another folder local. In kaldi/egs/code-switching/data/local create a file corpus.txt which should contain every single utterance transcription that can occur in your ASR system (in our case it will be 100 lines from 100 audio files).\n",
    "\n",
    "Pattern: [text_transcription]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_path = parent_path + \"/data/local\"\n",
    "if not os.path.exists(temp_path):\n",
    "    os.makedirs(temp_path)\n",
    "    \n",
    "corpus_path = parent_path + \"/data/local/corpus.txt\"\n",
    "trans_path = parent_path + \"/LDC2015S04/seame_d2/data/interview/transcript\"\n",
    "trans_list = os.listdir(trans_path)[1:]\n",
    "\n",
    "with open(corpus_path, 'w') as outputfile:\n",
    "    for file in trans_list: \n",
    "            trans_file = trans_path + \"/\" + file\n",
    "            with open(trans_file, 'r') as inputfile:\n",
    "                for line in inputfile:\n",
    "                    #outputfile.write(line)\n",
    "                    outputfile.write(re.split(\"\\t\", line)[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_path = parent_path + \"/data/local\"\n",
    "if not os.path.exists(temp_path):\n",
    "    os.makedirs(temp_path)\n",
    "    \n",
    "corpus_path = parent_path + \"/data/local/corpus.txt\"\n",
    "trans_path = parent_path + \"/LDC2015S04/seame_d1/data/conversation/transcript\"\n",
    "trans_list = os.listdir(trans_path)[1:]\n",
    "\n",
    "with open(corpus_path, 'a+') as outputfile:\n",
    "    for file in trans_list: \n",
    "            trans_file = trans_path + \"/\" + file\n",
    "            with open(trans_file, 'r') as inputfile:\n",
    "                for line in inputfile:\n",
    "                    #outputfile.write(line)\n",
    "                    outputfile.write(re.split(\"\\t\", line)[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trans_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "less ../data/local/corpus.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) segments file  \n",
    "the format of the \"segments\" file is:  \n",
    "[utterance-id] [recoding-id] [segment-begin] [segment-end]   \n",
    "string.split(\"\\_\", 2) # split up to the second of occurences of _\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# interview segments file for training set\n",
    "directory = parent_path + \"/data/train/segments\"\n",
    "counter = 0\n",
    "with open(directory, 'w') as outputfile:\n",
    "    for utt in utter_ids_i:\n",
    "        speaker_id = re.split(\"_\", utt)[0]\n",
    "        if speaker_id in train_ids_i:\n",
    "            \n",
    "            recording_id = \"_\".join(utt.split(\"_\", 2)[:2])\n",
    "            segment_begin = str(int(re.split(\"_\", utt)[2])/1000)\n",
    "            segment_end = str(int(re.split(\"_\", utt)[3])/1000)\n",
    "            outputfile.write(\"{} {} {} {}\\n\".format(utt, recording_id, segment_begin, segment_end))\n",
    "    \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NC01FBX_0101_0086300_0088370', 'NC01FBX_0101_0165090_0167860']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utter_ids_c[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conversation segments file for training set\n",
    "\n",
    "directory = parent_path + \"/data/train/segments\"\n",
    "counter = 0\n",
    "with open(directory, 'a+') as outputfile:\n",
    "    for utt in utter_ids_c:\n",
    "        speaker_id = re.split(\"_\", utt)[0]\n",
    "        if speaker_id in train_ids_c_short:\n",
    "            \n",
    "            recording_id = \"_\".join(utt.split(\"_\", 2)[:2])\n",
    "            segment_begin = str(int(re.split(\"_\", utt)[2])/1000)\n",
    "            segment_end = str(int(re.split(\"_\", utt)[3])/1000)\n",
    "            outputfile.write(\"{} {} {} {}\\n\".format(utt, recording_id, segment_begin, segment_end))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3652\n"
     ]
    }
   ],
   "source": [
    "# interview segments file for training set\n",
    "directory = parent_path + \"/data/test/segments\"\n",
    "counter = 0 \n",
    "with open(directory, 'w') as outputfile:\n",
    "    for utt in utter_ids_i:\n",
    "        speaker_id = re.split(\"_\", utt)[0]\n",
    "        if speaker_id in test_ids_i:\n",
    "#             if speaker_id == 'NI01MAX':\n",
    "#                 #print (speaker_id)\n",
    "            recording_id = \"_\".join(utt.split(\"_\", 2)[:2])\n",
    "            segment_begin = str(int(re.split(\"_\", utt)[2])/1000)\n",
    "            segment_end = str(int(re.split(\"_\", utt)[3])/1000)\n",
    "            outputfile.write(\"{} {} {} {}\\n\".format(utt, recording_id, segment_begin, segment_end))\n",
    "            counter += 1\n",
    "print (counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485\n"
     ]
    }
   ],
   "source": [
    "# conversation segments file for test set\n",
    "directory = parent_path + \"/data/test/segments\"\n",
    "counter = 0 \n",
    "with open(directory, 'a+') as outputfile:\n",
    "    for utt in utter_ids_c:\n",
    "        speaker_id = re.split(\"_\", utt)[0]\n",
    "        if speaker_id in test_ids_c_short:\n",
    "#             if speaker_id == 'NI01MAX':\n",
    "#                 #print (speaker_id)\n",
    "            recording_id = \"_\".join(utt.split(\"_\", 2)[:2])\n",
    "            segment_begin = str(int(re.split(\"_\", utt)[2])/1000)\n",
    "            segment_end = str(int(re.split(\"_\", utt)[3])/1000)\n",
    "            outputfile.write(\"{} {} {} {}\\n\".format(utt, recording_id, segment_begin, segment_end))\n",
    "            counter += 1\n",
    "print (counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "less ../data/train/segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "less ../data/test/segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NC01FBX_0101_0086300_0088370', 'NC01FBX_0101_0165090_0167860']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utter_ids_c[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
