{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from https://segmentfault.com/q/1010000000732038\n",
    "# to detect english character ONLY\n",
    "def isAlpha(word):\n",
    "    try:\n",
    "        return word.encode('ascii').isalpha()\n",
    "    except UnicodeEncodeError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from http://www.cnblogs.com/kaituorensheng/p/3554571.html\n",
    "# -*- coding: cp936 -*-\n",
    "def strQ2B(ustring):\n",
    "    \"\"\"全角转半角\"\"\"\n",
    "    rstring = \"\"\n",
    "    for uchar in ustring:\n",
    "        inside_code=ord(uchar)\n",
    "        if inside_code == 12288:                              #全角空格直接转换            \n",
    "            inside_code = 32 \n",
    "        elif (inside_code >= 65281 and inside_code <= 65374): #全角字符（除空格）根据关系转化\n",
    "            inside_code -= 65248\n",
    "\n",
    "        rstring += chr(inside_code)\n",
    "    return rstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loading cmu dictionary\n",
      "Number of English lexicon: 133854\n"
     ]
    }
   ],
   "source": [
    "filename_cmu = \"cmudict-0.7b\"\n",
    "\n",
    "d_cmu = dict()\n",
    "max_num = 0 # max number of (x) <-- a word with multiple pronunciations\n",
    "f = open(filename_cmu,\"r\")\n",
    "for line in f.readlines():\n",
    "    if line[0:3] != \";;;\":\n",
    "        line = line[:-1]\n",
    "        line = line.split(\"  \",1)\n",
    "        d_cmu[line[0]] = line[1]\n",
    "        if line[0][-1] == \")\":\n",
    "            try: \n",
    "                num_cur = int(line[0][-2])      \n",
    "                if int(line[0][-2]) > max_num:\n",
    "                    max_num = num_cur\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "print(\"Finish loading cmu dictionary\")\n",
    "print(\"Number of English lexicon: \" + str(len(d_cmu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loading thchs30 dictionary\n",
      "Number of Chinese lexicon: 8873\n"
     ]
    }
   ],
   "source": [
    "filename_th = \"thchs30-lexicon.txt\"\n",
    "\n",
    "d_th = dict()\n",
    "with codecs.open(filename_th, 'r', 'utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.split()\n",
    "        d_th[line[0]] = ' '.join(line[1:])\n",
    "print(\"Finish loading thchs30 dictionary\")\n",
    "print(\"Number of Chinese lexicon: \" + str(len(d_th)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfilename_thp = \"thchs30-lexiconp.txt\"\\n\\nd_thp = dict()\\nmax_num = 0\\n#f = open(filename_th,\"r\")\\nwith codecs.open(filename_th, \\'r\\', \\'utf-8\\') as f:\\n    for line in f.readlines():\\n        line = line.split()\\n        d_thp[line[0]] = \\' \\'.join(line[1:])\\nprint(len(d_thp))\\nprint(\"Finish loading thchs30 dictionary p\")\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the length of thchs30-lexiconp.txt is the same to thchs30-lexicon.txt\n",
    "# and the content looks same..\n",
    "'''\n",
    "filename_thp = \"thchs30-lexiconp.txt\"\n",
    "\n",
    "d_thp = dict()\n",
    "max_num = 0\n",
    "#f = open(filename_th,\"r\")\n",
    "with codecs.open(filename_th, 'r', 'utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.split()\n",
    "        d_thp[line[0]] = ' '.join(line[1:])\n",
    "print(len(d_thp))\n",
    "print(\"Finish loading thchs30 dictionary p\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish concatenating words\n",
      "Number of words: 518489\n",
      "Number of unique words: 28610\n"
     ]
    }
   ],
   "source": [
    "dir = \"./transcript\"\n",
    "\n",
    "# to get the full word list\n",
    "filenames = os.listdir(dir)\n",
    "words = []\n",
    "for filename in filenames:\n",
    "    if filename[-4:] == \".txt\":\n",
    "        with codecs.open(dir+\"/\"+filename, 'r', 'utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                sentence = line.split() # 1st element -- audio file id, 2nd element -- start time, 3rd element -- end time\n",
    "                words_cur = sentence[4:]   \n",
    "                words += words_cur\n",
    "            f.close()\n",
    "\n",
    "print(\"Finish concatenating words\")\n",
    "print(\"Number of words: \" + str(len(words)))\n",
    "print(\"Number of unique words: \" + str(len(set(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish creating waste word dictionary\n",
      "Number of waste word type: 168\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary of waste words\n",
    "words_waste = []\n",
    "words_1 = [] # [x]\n",
    "words_2 = [] # (x)\n",
    "        \n",
    "for word in words:\n",
    "    if word[-1] == \"]\" or word[0] == \"[\":\n",
    "        word = re.findall(r'''(\\[.+?\\])''',word)\n",
    "        words_1 += word\n",
    "    elif word[-1] == \")\" or word[0] == \"(\":\n",
    "        word = re.findall(r'''(\\(.+?\\))''',word)\n",
    "        words_2 += word\n",
    "        \n",
    "words_waste = words_1 + words_2\n",
    "\n",
    "d_waste = dict()\n",
    "for word in words_waste:\n",
    "    try:\n",
    "        d_waste[word] += 1\n",
    "    except:\n",
    "        d_waste[word] = 1\n",
    "\n",
    "d_waste2idx = dict()\n",
    "idx = -1\n",
    "for k in d_waste.keys():\n",
    "    idx += 1\n",
    "    d_waste2idx[k] = 'SIL' + str(idx)\n",
    "\n",
    "d_idx2waste = {v:k for k,v in d_waste2idx.items()}\n",
    "\n",
    "print(\"Finish creating waste word dictionary\")\n",
    "print(\"Number of waste word type: \" + str(len(set(words_waste))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish reading lines\n",
      "Number of segmented utterance: 45872\n",
      "Number of unsegmented utterance: 9057\n"
     ]
    }
   ],
   "source": [
    "# fix and output the transcript text going to be used\n",
    "dir = \"./transcript/\"\n",
    "dir_save = \"./transcript_fixed/\" \n",
    "filenames = os.listdir(dir)\n",
    "words_all = []\n",
    "text_all = []\n",
    "text_unparse = []\n",
    "for filename in filenames:\n",
    "    if filename[-4:] == \".txt\":\n",
    "        with codecs.open(dir+filename, 'r', 'utf-8') as f:\n",
    "            text = []\n",
    "\n",
    "            for line in f.readlines():\n",
    "                sentence_origin = line.strip()\n",
    "                \n",
    "                words_mm_cur = set(re.findall(r'''o(\\[.+?\\])''',line))\n",
    "                if len(words_mm_cur) > 0:\n",
    "                    sentence_origin = sentence_origin.replace('o[mm]','omm')\n",
    "                \n",
    "                # replace waste words with SILx; space after ] or before [ to avoid situations like co[mm]on or co[mm]unication\n",
    "                words_waste_cur = set(re.findall(r'''(\\[.+?\\])''',line) + re.findall(r'''(\\(.+?\\))''',line))\n",
    "                for element in words_waste_cur:\n",
    "                    try:\n",
    "                        sentence_origin = sentence_origin.replace(element,' ' + d_waste2idx[element] + ' ')\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # get rid of \"#\"\n",
    "                sentence_origin = sentence_origin.replace('#',' ')\n",
    "                \n",
    "                # get rid of \"=\"\n",
    "                sentence_origin = sentence_origin.replace('=',' ')\n",
    "                \n",
    "                # deal with %word% like %chelsia%\n",
    "                sentence_origin = sentence_origin.replace('%',' ')\n",
    "                \n",
    "                # deal with \"word\" like \"william\"\n",
    "                sentence_origin = sentence_origin.replace('\"',' ')\n",
    "                \n",
    "                # deal with pronunciation of single letter, for which the trascript is like P. S., I. T., etc.\n",
    "                sentence_origin = sentence_origin.replace('.',' ')\n",
    "                \n",
    "                # Q2B\n",
    "                sentence_chars = list(sentence_origin)\n",
    "                for char_idx in range(len(sentence_chars)):\n",
    "                    sentence_chars[char_idx] = strQ2B(sentence_chars[char_idx])\n",
    "                sentence_origin = \"\".join(sentence_chars)\n",
    "                \n",
    "                # trim again incase the substitution brings in space\n",
    "                sentence_origin = sentence_origin.strip()\n",
    "                              \n",
    "                sentence = sentence_origin.split() # 1st element -- audio file id, 2nd element -- start time, 3rd element -- end time\n",
    "                info_cur = sentence[:3] # idx: 0~2\n",
    "                words_cur = sentence[3:] # idx: 3~\n",
    "                unparse = False\n",
    "    \n",
    "                for word in words_cur: # filter out concatenated characters having Chinese characters, with length over 4\n",
    "          \n",
    "                    # filter out unparsed element (unparse = F -> unparse = T)\n",
    "                    # for example, [leh]每次上学的时候daddy都会买糕点给我吃买马来糕#kuih# (idx: 53628)\n",
    "                    try:\n",
    "                        d_cmu[word.upper()] # if the word is a recorded English word\n",
    "                        continue\n",
    "                    except:\n",
    "                        try:\n",
    "                            d_idx2waste[word] # if the word is SILx\n",
    "                            continue\n",
    "                        except:\n",
    "                            try:\n",
    "                                d_th[word] # if the word is a recorded Chinese word\n",
    "                                continue\n",
    "                            except:\n",
    "                                word_split = word.split(\"-\")\n",
    "                                if len(word_split) > 1: # pass if it is in the form of \"a-b\"\n",
    "                                    continue\n",
    "                                elif isAlpha(word.replace(\"'\",\"\")): # words like o'clock\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    if len(word) > 4 and isAlpha(word) == False:\n",
    "                                        unparse = True\n",
    "                                        break\n",
    "\n",
    "                if unparse:\n",
    "                    text_unparse += [sentence_origin]\n",
    "                else:\n",
    "                    sentence_cur = \" \".join(info_cur) + \" \" + \" \".join(words_cur)\n",
    "                    text += [sentence_cur]\n",
    "                    text_all += [sentence_cur]\n",
    "                    words_all += words_cur\n",
    "    \n",
    "            f.close()\n",
    "            \n",
    "        #save files without unparsed lines\n",
    "        with codecs.open(dir_save+filename, 'w', 'utf-8') as f:\n",
    "            for line in text:\n",
    "                f.write(line + \"\\n\")\n",
    "        f.close()\n",
    "\n",
    "with codecs.open(dir_save+'unparsed.txt', 'w', 'utf-8') as f:\n",
    "    for line in text_unparse:\n",
    "        f.write(line + \"\\n\")\n",
    "    f.close()\n",
    "\n",
    "with codecs.open(dir_save+'text.txt', 'w', 'utf-8') as f:\n",
    "    for line in text_all:\n",
    "        f.write(line + \"\\n\")\n",
    "    f.close()\n",
    "            \n",
    "print(\"Finish reading lines\")\n",
    "print(\"Number of segmented utterance: \"+str(len(text_all)))\n",
    "print(\"Number of unsegmented utterance: \"+str(len(text_unparse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish writing silence_phones.txt\n",
      "Finish writing nonsilence_phones.txt\n"
     ]
    }
   ],
   "source": [
    "# output silence_phones.txt\n",
    "with codecs.open(dir_save+'silence_phones.txt', 'w', 'utf-8') as f:\n",
    "    f.write('SIL\\n')\n",
    "    for k,v in d_waste2idx.items():\n",
    "        f.write(v + \"\\n\")\n",
    "f.close()\n",
    "print(\"Finish writing silence_phones.txt\")\n",
    "\n",
    "# output nonsilence_phones.txt\n",
    "phones = []\n",
    "\n",
    "with codecs.open('cmudict-0.7b.phones', 'r', 'utf-8') as f:\n",
    "    for phone in f.readlines():\n",
    "        phone = phone.split()\n",
    "        phones += [phone[0]]\n",
    "f.close()\n",
    "\n",
    "with codecs.open('thchs30-nonsilence_phones.txt', 'r', 'utf-8') as f:\n",
    "    for phone in f.readlines():\n",
    "        phones += [phone[:-1]] #negelect \\n\n",
    "f.close()\n",
    "\n",
    "with codecs.open(dir_save+'nonsilence_phones.txt', 'w', 'utf-8') as f:\n",
    "    for phone in phones:\n",
    "        f.write(phone + \"\\n\")\n",
    "f.close()\n",
    "\n",
    "print(\"Finish writing nonsilence_phones.txt\")\n",
    "#print(phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish lexicon filtering\n",
      "Number of words used: 534296\n",
      "Number of unique words used: 17531\n",
      "Number of unique words in filtered lexicon file: 11205\n"
     ]
    }
   ],
   "source": [
    "words_all = [word.upper() for word in words_all]\n",
    "words_all_uniq = list(set(words_all))\n",
    "words_oov = []\n",
    "d_train = dict()\n",
    "for word in words_all_uniq:\n",
    "    try:\n",
    "        d_train[word] = d_cmu[word]\n",
    "        for i in range(1,max_num+1):\n",
    "            try:\n",
    "                d_train[word+\"(i)\"] = d[word+\"(i)\"]\n",
    "            except:\n",
    "                break\n",
    "    except:\n",
    "        try:\n",
    "            d_train[word] = d_th[word]\n",
    "        except:\n",
    "            words_oov += [word]\n",
    "            continue\n",
    "print(\"Finish lexicon filtering\")\n",
    "print(\"Number of words used: \" + str(len(words_all)))\n",
    "print(\"Number of unique words used: \" + str(len(words_all_uniq)))\n",
    "print(\"Number of unique words in filtered lexicon file: \" + str(len(d_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish writing lexicon.txt\n"
     ]
    }
   ],
   "source": [
    "# output lexicon.txt\n",
    "lexicons = [str(k) + \" \" + str(v) + \"\\n\" for k,v in d_train.items()]\n",
    "\n",
    "with codecs.open(dir_save+'lexicon.txt', 'w', 'utf-8') as f:\n",
    "    f.write('<oov> <oov>\\n')\n",
    "    for lexicon in lexicons:\n",
    "        f.write(lexicon)\n",
    "    #for word in words_oov:\n",
    "    #    f.write(str(word) + \" \" + \"<oov>\\n\")\n",
    "    for k,v in d_waste2idx.items():\n",
    "        f.write(v + \" sil\" + \"\\n\")\n",
    "        \n",
    "f.close()\n",
    "\n",
    "print(\"Finish writing lexicon.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NI61FBP_0101\\t696201\\t702891\\t有  以前  我 sec one sec two 就 用 里 谷 join 然后 sec three sec four 有 join 那个 robocup\\n'"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[32788] # cannot be understood by human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NI22FBP_0101\\t668052\\t669342\\t他  对  她  有  ·䞠\\x84\\x9f枠§\\x89\\n'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[19959] # gibberlish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NI22FBP_0101\\t1040195\\t1044030\\tthen 他  会  给 ten dollar #angpow ha ha ha\\n'"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[20115] # loss of label: \"#angpow\" should be \"#angpow#\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UI17FAZ_0103\\t751720\\t754727\\tah sorry我不吃#kim gary#的 呵呵呵\\n'"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[46226] # space between labeled term: if we use the default split method, #kim gary# will be split into 2 words\n",
    "            # as the number of space between is 1 between english words and 2 between chinese words, it's really troublesome \n",
    "            # to do right split to avoid this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NI24MBP_0101\\t1138380\\t1140790\\t我  我  不是  去 %instep% 我是  去 g. i. p.\\n'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[21298] # % indicates a special noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NI25MBQ_0101\\t583590\\t587390\\t所以 %zhi=hong% %zhi=hong% 有点 %zhi=hong% was very stress because er\\n'"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[21373] # % indicates a special noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NI26FBP_0101\\t1073610\\t1076900\\tit was then %sweety% decided that 他  会 take up cultural\\n'"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[21503] # % indicates a special noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NI26FBP_0101\\t1261440\\t1262820\\t回来  继续  玩 %viwawa%\\n'"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[21513] # % indicates a special noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NI43FBP_0101\\t1494840\\t1501690\\t不错  蛮  很好  吃  的  那边  的 %sashimi% 是  很  新鲜  的  那一  种  啊  就  一直  吃  \"sashimi\"\\n'"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[27698] # inconsistent annotation: according to the codebook, \"sashimi\" shoud be annotated as \"#sashimi#\" \n",
    "            # because it's a japanese word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UI21MAZ_0101\\t689491\\t700786\\t可能出来我发现我是不喜欢 i. t. 的 那时候如果现在是我工作我又有钱赚然后有可以得到经验如果又知道是 o. k. 的话到时半工读啰会比较好啦[mmm] \\n'"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[48495] # wrongly split a word in transcription: \"i. t.\" should be \"IT\", \"o. k.\" should be \"ok\" in our sense \n",
    "            # it may help if we discard the \".\" after the character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UI29FAZ_0102\\t64704\\t71545\\t又有m.m.lah 我的我的有的有的我都不懂啊知道最著名的是e.p.i.k.o. [ah] h.r. [ah] err\\n'"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[54294] # need to check the audio to see if \"epiko\" is pronunced as a word or letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UI28FAZ_0101\\t221989\\t230002\\t大嫂干净[mmm] move 去那个新#taman#之后新的garden\\n'"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[53626] # wrong transcription of mandarin: \"大嫂干净\" should be \"打扫干净\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UI27FAZ_0103\\t1112733\\t1119724\\t啊touch screen的 and 通常会有人会买i. pad compared to netbook\\n'"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[53390] # wrong transcription of english: \"netbook\" should be \"notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UI16MAZ_0103\\t747433\\t752878\\t哦那个我还不能啦 但是我？了起来大约大约想想这样才可以啦 \\n'"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[45496] # wrong annotation: \"?\" might refer to a non-transcribable sound?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UI27FAZ_0101\\t752877\\t756798\\tw. h. o.是world world health organize organization\\n'"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[52996] # for this kind of unparsed utterance, we can split chinese charater and english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "埃塞俄比亚\n",
      "多斯桑托斯\n",
      "<SPOKEN_NOISE>\n",
      "人民大会堂\n",
      "一九九二年\n",
      "五千四百五十\n",
      "聪明反被聪明误\n",
      "乌兹别克斯坦\n",
      "百分之二十\n",
      "四百二十三\n",
      "塞尔维亚人\n",
      "原因很简单\n",
      "中华人民共和国\n",
      "幸福的家庭\n",
      "中国科学院\n",
      "一千二百二十七\n",
      "叫我怎么办\n",
      "二百九十一\n",
      "三思而后行\n",
      "艾森豪威尔\n",
      "四千五百七十\n",
      "二百五十五\n",
      "一百七十四\n",
      "三百五十三\n",
      "自己的房间\n",
      "一百三十三\n",
      "新概念英语\n",
      "阿拉伯数字\n",
      "二十一世纪\n",
      "一百三十四\n",
      "三千四百五十\n",
      "五万五千五百五十五\n",
      "一百三十五\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for k in d_th.keys():\n",
    "    if len(k) > 4:\n",
    "        print(k)\n",
    "        i+=1\n",
    "print(i)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
