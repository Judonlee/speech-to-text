### How to train our language model

Summary: in [QingHua's paper](https://arxiv.org/pdf/1609.08412v1.pdf) Their best Lanuage model with lowest WER (MIX CH: 19.00 EN: 43.67 Overall:20.09) comes from a linear interpolation of OCLM (trained from the transcriptions of the training dataset) and THLM (LM offered by THCHS30 database). The second lowest language model with WER(CH: 19.30 EN: 43.86 Overall: 20.37) was trained from all the transcriptions of THCHS30 and OC16-CE80. Summ
>As for the language model (LM), we used the conventional 3-grams. To deal with the mixlingual difficulty, four  configuration were investigated. The first one (THLM) was the LM offered by the THCHS30 database. This LM well matched the lexicon of the baseline system, as the Chinese words in the lexicon were just from THCHS30. The problem of this LM was that it involved no English words so it could not handle English at all. The second LM (OCLM) was trained from the transcriptions of the training and development data of OC16-CE80. This LM matched the test domain and involved English words, however it might suffer from data sparsity. The third LM (MIX) was a mixture of the above two LMs by a linear interpolation, where the validation set used to optimize the interpolation factor was 2, 000 sentences selected from the transcriptions of the OC16-CE80 database. In our experiment, we found the interpolation factor for the THLM was nearly 0.0, suggesting that THLM does not fit the domain of the OC16-CE80 database. The fourth LM (JOIN) was trained with all the transcriptions of THCHS30 and OC16-CE80. Note that the baseline lexicon was used when training the OCLM and the JOIN LM, which meant that all the words in the lexicon were consisted in these LMs, although the words absent from the training text data obtained only a small probability, depending on the KN smooth we used in the experiment.
