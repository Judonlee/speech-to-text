### How to train our language model

As for the language model (LM), we used the conventional
3-grams. To deal with the mixlingual difficulty, four LM
configurations were investigated. The first one (THLM) was
the LM offered by the THCHS30 database. This LM well
matched the lexicon of the baseline system, as the Chinese
words in the lexicon were just from THCHS30. The problem
of this LM was that it involved no English words so it could
not handle English at all. The second LM (OCLM) was trained
from the transcriptions of the training and development data of
OC16-CE80. This LM matched the test domain and involved
English words, however it might suffer from data sparsity. The
third LM (MIX) was a mixture of the above two LMs by a
linear interpolation, where the validation set used to optimize
the interpolation factor was 2, 000 sentences selected from the
transcriptions of the OC16-CE80 database. In our experiment,
we found the interpolation factor for the THLM was nearly
0.0, suggesting that THLM does not fit the domain of the
OC16-CE80 database. The fourth LM (JOIN) was trained with
all the transcriptions of THCHS30 and OC16-CE80. Note that
the baseline lexicon was used when training the OCLM and the
JOIN LM, which meant that all the words in the lexicon were
consisted in these LMs, although the words absent from the
training text data obtained only a small probability, depending
on the KN smooth we used in the experiment.
